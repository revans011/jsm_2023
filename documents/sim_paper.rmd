---
output:
    html_document: default
    pdf_document:
      keep_tex: true
geometry: margin=1.0in
font-size: 12pt
header-includes:
    - \usepackage{helvet}
    - \renewcommand*\familydefault{\sfdefault}
    - \usepackage{setspace}
    - \doublespacing
    - \usepackage[left]{lineno}
    - \linenumbers
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}

## Install these libraries before knitting

library("here")
library("tinytex")
library("knitr")
library("kableExtra")
library("rmarkdown")
library("dplyr")
library("kableExtra")
library("knitr")

tab1 <- read.csv(here("data","derived","bigtable.csv"))

# render(here("documents/sim_paper.rmd"), output_format = "all")
```

# Sample Size Considerations in the Design of Orthopaedic Risk-factor Studies

**Running title:** Sample Size Considerations


Richard Evans ${^\dagger}$

${\dagger}$ Corresponding Author

Masonic Cancer Center  
University of Minnesota  

\href{mailto:evan0770@umn.edu}{evan0770@umn.edu}

\newpage

## Abstract

### Objective

In orthopaedic risk-factor studies, the disease status of control subjects is not always perfectly ascertained. That means control groups may be mixtures of both unaffected cases and some unidentified affected cases. Control groups with misclassified data are called _unlabeled_. Treating unlabeled groups as disease-negative control groups is known to cause misclassification bias, but there has been little research on how the misclassification affects the statistical power of the risk association tests. In this study, we investigated the effect using unlabeled groups as control groups on the power of association tests, with the intention of demonstrating that control groups with even small misclassification rates can reduce the power of association tests.

### Materials and Methods

This was a simulation study using study designs from published orthopaedic risk-factor studies. The approach was to use their designs, but simulate the data to include known proportions of misclassified affected subjects in the control group. The simulated data was used to calculate the power of the risk-association test. We calculated powers for several study designs and misclassification rates, and compared them to a reference model, which is the case then there are no misclassifications and the model is correct.

### Results

Treating unlabeled data as disease-negative always reduced statistical power compared to the reference power. Power loss was greater with increasing misclassification rate. Increasing the sample size by a factor of 1.4 was adequate to regain power.

### Conclusion

Researchers calculating sample sizes for risk-factor studies should include adjustments for misclassification rates.

Keywords: risk-factor, case-control, power, sample size


\newpage

## Introduction

In risk factor studies, the positive disease status of the affected subjects is ascertained with perfect sensitivity and specificity, but sometimes the disease status of control subjects is not perfectly ascertained. That means control groups may be mixtures of both unaffected cases and some unidentified affected cases. Control groups with misclassified data are called _unlabeled_. Data with truly affected cases in the positive group and an unlabeled control group is called _positive-unlabeled_ data by the data science community.

Examples of positive-unlabeled data are well documented in human medicine, but less so in veterinary medicine. Nevertheless, many veterinary studies the fall into the positive-unlabeled framework. For example, genome-wide association studies of cranial cruciate ligament disease (CCLD) in dogs use case-control designs. The affected cases are truly positive CCLD cases because they are enrolled from the set of dogs who have undergone knee stabilization surgery. The controls are typically five-years-old or older with no history of CCLD and pass an orthopedic veterinary exam by a board-certified surgeon. However some control dogs will have spontaneous rupture in the future, and so genetically belong in the CCLD affected group. Other control dogs may have sub-diagnostic disease. For example, a dog might appear sound on physical exam and be enrolled in the control group, but may actually have force-platform-detectable hindlimb lameness. Such a dog should not be in the control group because the lameness might be subclinical CCLD. \cite{wrehim08}

There are other examples of PU data in the veterinary literature, typically in risk-factor studies using case-control designs. For example, Arthur et al. (2016) used a case-control design to assess the risk of osteosarcoma following fracture repair. \cite{aakj16} They noted, "There may be additional cases [in the control group] in which implant-related osteosarcoma was diagnosed in the private practice setting without referral...," suggesting that the control group may be unlabeled because some control-group cases were actually disease positive, but diagnosed outside the study. In another example, Wylie et al. 2013 studied risk factors for equine laminitis using controls obtained from an owner survey.\cite{wcvj13} The authors noted the positive-unlabeled aspect of their data, "Our study relied on owner-reported diagnoses of endocrinopathic conditions, and this may have introduced misclassification bias."

As mentioned above, the affected cases are "labeled" positive, but the control data is "unlabeled," because dogs may be affected or unaffected. Treating the unlabeled control group as entirely unaffected is called the _naive model_. The proportion of affected dogs in the control group is called the _nondetection rate_ or _undetected rate_.

Using the naive model when the nondetection rate is positive causes misclassification bias (because there affected cases in the control group), and that bias is well documented in the data science literature. \cite{bd20} The biases due to misclassification can be mitigated using models other than the naive model and with the appropriate data analysis, and there are many articles describing methods for analyzing positive-unlabeled data. Bekker provides and excellent summary of methods. Sometimes, however, researchers prefer the naive model because they believe that their small nondetection rates induce misclassification biases that are too small for practical consideration. There is some suggestion that nondetection rates under 10% do have little impact on bias. \cite{bd20}

But bias in estimates (e.g., bias in regression coefficients) is just one part of the results; the other part is inference (e.g., p-values). Central to inference is the power of statistical tests. Power is used in planning a study as a measure of the ability of the study to make the correct decisions. That is, finding P<0.05 when it should. Typically, 80 percent power means that if the group parameters are truly different, then the statistical test has an 80 percent chance of obtaining p<0.05.

During the design phase phase of a risk association study, researchers might calculate the sample size they need for 80% power assuming the nondetection rate is zero. That is, the there are no misclassified affected subjects in the control group. However, if after collection the data are positive unlabeled, then the naive model is incorrect and the estimated power may be less than estimated.

We investigated the effect of positive-unlabeled data on loss of statistical power under the naive model. For comparision, the reference power is defined as the power when the naive model is correct and the group sizes are balanced. The results are described in terms power loss relative to the reference power, both percent power loss and absolute power loss. For context these two quantities are analogous to relative risk and absolute risk from epidemiology.

Using a simulation, we described how statistical power changes with varying proportions of undetected positives in the naive controls, and varying the imbalance between the numbers of cases and naive controls. Our first aim was to demonstrate that the naive analysis of positive-unlabeled data reduces statistical power in risk-factor studies, even for small nondetection rates. Our second aim was to offer a correction factor to upward-adjust samples sizes and correct for the power loss described in aim 1.

\newpage

## Methods and materials

### The Test of Association

This was a simulation study assessing the changes in power of a univariate association test under different PU conditions. There are many statistical tests of association, but we calculated the power for one of the most common tests, the Fisher exact test, which is used to test statistical significance of a binary risk factor. More generally, this test can be used to assess the significance of any risk factor using the predicted values from a univariate logistic regression.

In the context of risk association studies, and all else being equal, the Fisher exact test would achieve its maximum power for a balanced study design when the naive model is correct (i.e., no undetected positives in the control group). We call that maximum power the _reference power_ and reported our results as both percent power loss relative to the reference power and as absolute power loss from reference power. In other words, we are using the Fisher exact test to show how much statistical power is lost by ignoring the nondetection rate.

### The Sample Size and Group Imbalance

The total sample size for the simulation was fixed N=200, which is consistent with Healey et al. 2019 (N=216), and Baird et al. 2014 (N=217). \cite{bcioa14} \cite{hmhcbhkr19} The effect size, 0.21, was chosen because with N=200, the reference power was close to 80 percent, which is value that is commonly used in study design. That way, the reference model is the one with standard power of 80 percent. Note that the sample size and effect size are not a key parameters for the simulation because for any sample size an effect size can be chosen so that power is 80 percent. Also, effect size and sample size are not features of PU data, per se.

The simulation study varied two study design parameters: the nondetection rate and group-size imbalance. The proportion of undetected positives in the control group ranged from 0 (the value for reference power) to 10 percent. We used 10 percent as the upper limit because researchers are generally willing to accept nondetection rates below 10 percent and use the naive model, but change to a PU analysis for rates greater than 10 percent. \cite{bd20}  

We modeled group imbalance using Healey et al. (2019), which used 161 dogs affected with CCLD and 55 unlabeled dogs as controls, and Baird et al. (2014) which used 91 dogs affected with CCLD, and 126 unlabeled dogs as controls, so that imbalance ratios were about 3:1 and 1:3.  \cite{bcioa14} \cite{hmhcbhkr19} We only used two imbalance proportions (1:3 and 3:1) and no imbalance (1:1) because the key parameter for this study was the nondetection proportion. That gave simulation sample sizes of (50, 150), (150, 50), and (100, 100).


\pagebreak

### The Simulation Algorithm

The overall approach is to simulate data, and then use that data to calculate the p-value of the Fisher exact test. The process is repeated 5000 times, for each combination of sample size and nondetection rate, and then the 5000 p-values are compared to 0.05. The proportion of p-values less than 0.05 is the estimated power.

Simulating the data works backward from what might be expected. Instead starting with values for a risk factor (e.g., 200 0's and 1's representing sex) and then simulating their disease status, we start with the disease status (e.g., 50 affected cases and 150 controls with 135 unaffected and 15 affected) and then assign binary values for the risk factor. It was done that way to control the nondetection rate and group sizes.

The simulation algorithm is most easily described using examples, and we begin with calculating power for the Fisher exact test under the reference model, which is 100 cases and 100 correctly labeled (i.e., truly unaffected) controls. That is, there are no affected cases in the control group, so this is not positive-unlabeled data, and the naive model is the correct model. Next we associate a binary risk factor variable, $X$ (e.g., sex), with the cases and controls.The negative controls were simulated by sampling 100 negative cases from a binomial distribution with $Pr(X=1) = 0.2$. That probability means the the baseline risk for the disease in the population is 0.2. It it was chosen arbitrarily, because the baseline risk isn't central to power, the effect size is. As mentioned above, the effect size was 0.21, so the 100 positive cases were sampled from a binomial distribution with $Pr(X=1) = 0.2 + 0.21$.

Now, the 200 cases are pairs of binary data, one representing the group and the other representing the risk factor. These simulated data were tested with the Fisher exact test. As mentioned above, this process was repeated 5000 times and the resulting 5000 p-values used to estimate power.

For the second example, we calculate the power for a positive-unlabeled example. Suppose that in a 100-patient control group, 10 percent are in fact undetected positives. So the dataset is 100 affected cases in the positive group, 10 affected cases in the control group, and 90 unaffected cases in the control group. As in the previous example, the risk factor is simulated by sampling from binomial distributions. Now, 90 controls are sampled from the binomial distribution with $Pr(X=1) = 0.2$, the 10 affected controls are sampled from the binomial distribution with $Pr(X=1) = 0.2+ 0.21$, and 100 cases are sampled from the same binomial distribution with $Pr(X=1) = 0.2 + 0.21$. The 10 mislabeled affected cases remain in the control group, so as to measure the effect of treating PU data naively. As before, the simulated data are treated like pilot data and power was calculated. This process is repeated 5000 times and the result averaged.

### The Correction Factor

The simulation algorithm was used to estimate the power of a Fisher's exact test for a specific effect size for different sample sizes. For aim 2, the sample-size correction factor, we used the same simulation algorithm and effect size (0.21) but multiplied the group sample sizes by possible correction factors, 1.1, 1.2, and so on, increasing sample size and the power until it reached the 80%.  

\pagebreak

## Results

Table 1 describes power loss for the three sample sizes, (50, 150), (150, 50), and (100, 100), and for three nondetection rates, 0, 0.05 (5%), and 0.1 (10%). To give these parameters context, if the group sizes are (50, 150) and the nondectection rate is 0.1, then the positive (affected) group has 50 cases, and the unlabeled control group (which we are treating naively) has 150 cases, 15 of which are actually affected cases. When the nondetection rate is zero, then the naive model is correct because there no affected cases in the control group. The first row is the reference power, so its loss of power compared to itself is zero. The reference power was calculated in the simulation just like all the other powers, and was estimated to be 0.82.

Columns 5 and 6 are the power loss columns and have negative entries because for this simulation positive-unlabeled data analyzed under the naive model always had lower power, as did unbalanced data. For example, the second row shows a `r round(tab1$rel.loss[2],2)`% relative power reduction when the group sizes are balanced but five percent of the control group is actually positive cases.

Using Table 1, increasing nondetection rate typically reduces power. However, group size imbalance can affect power loss more than relatively small nondetection rates. For example, consider rows 3, 4, and 5. For those rows, the nondetection rate is decreasing to zero, but power loss is increasing due to group imbalance. However, within an experimented with fixed group sizes (e.g., 100,100) increasing nondetection rate alway means decreased power.

Table 1. Power loss.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

kable(tab1,
      format="markdown",
      digits=2,
      col.names = c("Row", 
                    "N positive cases", 
                    "N naive controls", 
                    "nondetection proportion", 
                    "Relative power loss (%)",
                    "Absolute power loss (from 0.82)"
                    )) |>
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 11,
                position = "left")
```

Table 2. Power improvement.
This table shows that increasing sample sizes by about 40% brings power back to the nominal rate. Rows 1, 6, and 11, show the power for when there are no false positives. The other rows show power when there is a 10% nondection rate. The sixth column shows the percent increase in sample size. 
```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}

tab2 <- read.csv(here("data","derived","bigtable_inc_ss.csv"))

tab2 <- tab2 %>% 
  select(X,n.cases,n.naiv.control,prp.nondet,naive.power) %>% 
  mutate(total = n.cases + n.naiv.control) %>% 
  relocate(total,.after = n.naiv.control) %>% 
  mutate(n.percent = 100* total/200 - 100) %>% 
  relocate(n.percent, .after=total) %>% 
  mutate(false.neg=round(prp.nondet*n.naiv.control)) %>% 
  relocate(false.neg, .after = n.naiv.control) %>% select(-prp.nondet)

kable(tab2,
      format="latex",
      digits=2,
      col.names = c("Row", 
                    "N positive cases", 
                    "N naive controls", 
                    "N false controls",
                    "N total",
                    "percent increase in N",
                    "Power"
                    )) |>
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 11,
                position = "left",
                latex_options="scale_down") %>% 
  row_spec(1,bold = TRUE)

```

\pagebreak

# Discussion

Aim This study showed that under specific conditions there was modest power loss even for small proportions of undetected positives in the control group. 


It also showed that for unbalanced designs, substantial increases in sample size were required to have 

The working example was the association test for a single SNP in a GWAS study, but the simulation results apply to any kind of study with univariate association tests, such as any risk factor study. That is a broad class of studies. Examples include the univariate associations between post-op surgical infections and various surgical conditions (e.g., boarded surgeon vs resident, manufacturer of bone plate). In that case, there may be subdiagnostic infections in the control group. Another example is univariate association tests in veterinary surveys, such as XXX. In that case, there may be subclinical ...

In this simulation, the undetected positives in the negative group were randomly sampled from the same population as the detected positives in the affected group. That's a common assumption (ref GU and others) but there are other models. For example, the undetected positives in the control group might be a subpopulation of positives defined by another variable. For example, in a CCLD GWAS study, the undetected positives in the control group might be positive dogs with low BCS only. 



 it does not apply to the situation where binary "affectedness" changes as the function of a scaled risk factor variable. Using risk factors for CCLD example, BCS may affect spontaneous rupture. 

This was a simulation study that considered the effect on statistical power of 10 percent or fewer undetected positives in the control group. The simulation used 

Other papers have discussed improved tests and power. (wang, and the pan paper )

\newpage

### Practical implications

However, the undetected-positive rate for CCLD GWAS studies is small, certainly less that 10 percent, and the low rate causes only small biases in odds ratio estimates. However, this study shows that unlabeled data also affect the inferences, and we show that even a few positive cases in the negative controls can affect the power of the study. Fortunately, positive-unlabeled data appears to reduce power, making the results reported in CCLD GWAS studies conservative.


\bibliography{references}
\bibliographystyle{plain}

## Appendix 1.
Table 1. Sample sizes for four GWAS studies of CCLD. The last columns shows the number of affected cases in the control group assuming 10 percent non-detection rate. 

|   Study |   N   | Cases | Naive controls | Max No. undetected positives (10%) |
|:-------------------:|:-----:|:-----:|:--------------:|:-------------:|
| Baird et al. (2014)     | 217   | 91    | 126       |  13     |
| Baker et al. (2021)     | 397   | 156   | 241       |  24     |
| Cook et al. (2020)      | 333   | 190   | 143       |  14     |
| Healy et al. (2019)     | 216   | 161   | 55        |  6      |



