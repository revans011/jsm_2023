% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1.0in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{helvet}
\renewcommand*\familydefault{\sfdefault}
\usepackage{setspace}
\doublespacing
\usepackage[left]{lineno}
\linenumbers
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\hypertarget{sample-size-considerations-in-the-design-of-orthopaedic-risk-factor-studies}{%
\section{Sample Size Considerations in the Design of Orthopaedic
Risk-factor
Studies}\label{sample-size-considerations-in-the-design-of-orthopaedic-risk-factor-studies}}

\textbf{Running title:} Sample Size Considerations

Richard Evans \({^\dagger}\)

\({\dagger}\) Corresponding Author

Masonic Cancer Center\\
University of Minnesota

\href{mailto:evan0770@umn.edu}{evan0770@umn.edu}

\newpage

\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

\hypertarget{objective}{%
\subsubsection{Objective}\label{objective}}

In orthopaedic risk-factor studies, the disease status of control
subjects is not always perfectly ascertained. That means control groups
may be mixtures of both unaffected cases and some unidentified affected
cases. Control groups with misclassified data are called
\emph{unlabeled}. Treating unlabeled groups as disease-negative control
groups is known to cause misclassification bias, but there has been
little research on how the misclassification affects the statistical
power of the risk association tests. In this study, we investigated the
effect using unlabeled groups as control groups on the power of
association tests, with the intention of demonstrating that control
groups with even small misclassification rates can reduce the power of
association tests.

\hypertarget{materials-and-methods}{%
\subsubsection{Materials and Methods}\label{materials-and-methods}}

This was a simulation study using study designs from published
orthopaedic risk-factor studies. The approach was to use their designs,
but simulate the data to include known proportions of misclassified
affected subjects in the control group. The simulated data was used to
calculate the power of the risk-association test. We calculated powers
for several study designs and misclassification rates, and compared them
to a reference model, which is the case then there are no
misclassifications and the model is correct.

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

Treating unlabeled data as disease-negative always reduced statistical
power compared to the reference power. Power loss was greater with
increasing misclassification rate. Increasing the sample size by a
factor of 1.4 was adequate to regain power.

\hypertarget{conclusion}{%
\subsubsection{Conclusion}\label{conclusion}}

Researchers calculating sample sizes for risk-factor studies should
include adjustments for misclassification rates.

Keywords: risk-factor, case-control, power, sample size

\newpage

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

In risk factor studies, the positive disease status of the affected
subjects is ascertained with perfect sensitivity and specificity, but
sometimes the disease status of control subjects is not perfectly
ascertained. That means control groups may be mixtures of both
unaffected cases and some unidentified affected cases. Control groups
with misclassified data are called \emph{unlabeled}. Data with truly
affected cases in the positive group and an unlabeled control group is
called \emph{positive-unlabeled} data by the data science community.

Examples of positive-unlabeled data are well documented in human
medicine, but less so in veterinary medicine. Nevertheless, many
veterinary studies the fall into the positive-unlabeled framework. For
example, genome-wide association studies of cranial cruciate ligament
disease (CCLD) in dogs use case-control designs. The affected cases are
truly positive CCLD cases because they are enrolled from the set of dogs
who have undergone knee stabilization surgery. The controls are
typically five-years-old or older with no history of CCLD and pass an
orthopedic veterinary exam by a board-certified surgeon. However some
control dogs will have spontaneous rupture in the future, and so
genetically belong in the CCLD affected group. Other control dogs may
have sub-diagnostic disease. For example, a dog might appear sound on
physical exam and be enrolled in the control group, but may actually
have force-platform-detectable hindlimb lameness. Such a dog should not
be in the control group because the lameness might be subclinical CCLD.
\cite{wrehim08}

There are other examples of PU data in the veterinary literature,
typically in risk-factor studies using case-control designs. For
example, Arthur et al.~(2016) used a case-control design to assess the
risk of osteosarcoma following fracture repair. \cite{aakj16} They
noted, ``There may be additional cases {[}in the control group{]} in
which implant-related osteosarcoma was diagnosed in the private practice
setting without referral\ldots,'' suggesting that the control group may
be unlabeled because some control-group cases were actually disease
positive, but diagnosed outside the study. In another example, Wylie et
al.~2013 studied risk factors for equine laminitis using controls
obtained from an owner survey.\cite{wcvj13} The authors noted the
positive-unlabeled aspect of their data, ``Our study relied on
owner-reported diagnoses of endocrinopathic conditions, and this may
have introduced misclassification bias.''

As mentioned above, the affected cases are ``labeled'' positive, but the
control data is ``unlabeled,'' because dogs may be affected or
unaffected. Treating the unlabeled control group as entirely unaffected
is called the \emph{naive model}. The proportion of affected dogs in the
control group is called the \emph{nondetection rate} or \emph{undetected
rate}.

Using the naive model when the nondetection rate is positive causes
misclassification bias (because there affected cases in the control
group), and that bias is well documented in the data science literature.
\cite{bd20} The biases due to misclassification can be mitigated using
models other than the naive model and with the appropriate data
analysis, and there are many articles describing methods for analyzing
positive-unlabeled data. Bekker provides and excellent summary of
methods. Sometimes, however, researchers prefer the naive model because
they believe that their small nondetection rates induce
misclassification biases that are too small for practical consideration.
There is some suggestion that nondetection rates under 10\% do have
little impact on bias. \cite{bd20}

But bias in estimates (e.g., bias in regression coefficients) is just
one part of the results; the other part is inference (e.g., p-values).
Central to inference is the power of statistical tests. Power is used in
planning a study as a measure of the ability of the study to make the
correct decisions. That is, finding P\textless0.05 when it should.
Typically, 80 percent power means that if the group parameters are truly
different, then the statistical test has an 80 percent chance of
obtaining p\textless0.05.

During the design phase phase of a risk association study, researchers
might calculate the sample size they need for 80\% power assuming the
nondetection rate is zero. That is, the there are no misclassified
affected subjects in the control group. However, if after collection the
data are positive unlabeled, then the naive model is incorrect and the
estimated power may be less than estimated.

We investigated the effect of positive-unlabeled data on loss of
statistical power under the naive model. For comparision, the reference
power is defined as the power when the naive model is correct and the
group sizes are balanced. The results are described in terms power loss
relative to the reference power, both percent power loss and absolute
power loss. For context these two quantities are analogous to relative
risk and absolute risk from epidemiology.

Using a simulation, we described how statistical power changes with
varying proportions of undetected positives in the naive controls, and
varying the imbalance between the numbers of cases and naive controls.
Our first aim was to demonstrate that the naive analysis of
positive-unlabeled data reduces statistical power in risk-factor
studies, even for small nondetection rates. Our second aim was to offer
a correction factor to upward-adjust samples sizes and correct for the
power loss described in aim 1.

\newpage

\hypertarget{methods-and-materials}{%
\subsection{Methods and materials}\label{methods-and-materials}}

\hypertarget{the-test-of-association}{%
\subsubsection{The Test of Association}\label{the-test-of-association}}

This was a simulation study assessing the changes in power of a
univariate association test under different PU conditions. There are
many statistical tests of association, but we calculated the power for
one of the most common tests, the Fisher exact test, which is used to
test statistical significance of a binary risk factor. More generally,
this test can be used to assess the significance of any risk factor
using the predicted values from a univariate logistic regression.

In the context of risk association studies, and all else being equal,
the Fisher exact test would achieve its maximum power for a balanced
study design when the naive model is correct (i.e., no undetected
positives in the control group). We call that maximum power the
\emph{reference power} and reported our results as both percent power
loss relative to the reference power and as absolute power loss from
reference power. In other words, we are using the Fisher exact test to
show how much statistical power is lost by ignoring the nondetection
rate.

\hypertarget{the-sample-size-and-group-imbalance}{%
\subsubsection{The Sample Size and Group
Imbalance}\label{the-sample-size-and-group-imbalance}}

The total sample size for the simulation was fixed N=200, which is
consistent with Healey et al.~2019 (N=216), and Baird et al.~2014
(N=217). \cite{bcioa14} \cite{hmhcbhkr19} The effect size, 0.21, was
chosen because with N=200, the reference power was close to 80 percent,
which is value that is commonly used in study design. That way, the
reference model is the one with standard power of 80 percent. Note that
the sample size and effect size are not a key parameters for the
simulation because for any sample size an effect size can be chosen so
that power is 80 percent. Also, effect size and sample size are not
features of PU data, per se.

The simulation study varied two study design parameters: the
nondetection rate and group-size imbalance. The proportion of undetected
positives in the control group ranged from 0 (the value for reference
power) to 10 percent. We used 10 percent as the upper limit because
researchers are generally willing to accept nondetection rates below 10
percent and use the naive model, but change to a PU analysis for rates
greater than 10 percent. \cite{bd20}

We modeled group imbalance using Healey et al.~(2019), which used 161
dogs affected with CCLD and 55 unlabeled dogs as controls, and Baird et
al.~(2014) which used 91 dogs affected with CCLD, and 126 unlabeled dogs
as controls, so that imbalance ratios were about 3:1 and 1:3.
\cite{bcioa14} \cite{hmhcbhkr19} We only used two imbalance proportions
(1:3 and 3:1) and no imbalance (1:1) because the key parameter for this
study was the nondetection proportion. That gave simulation sample sizes
of (50, 150), (150, 50), and (100, 100).

\pagebreak

\hypertarget{the-simulation-algorithm}{%
\subsubsection{The Simulation
Algorithm}\label{the-simulation-algorithm}}

The overall approach is to simulate data, and then use that data to
calculate the p-value of the Fisher exact test. The process is repeated
5000 times, for each combination of sample size and nondetection rate,
and then the 5000 p-values are compared to 0.05. The proportion of
p-values less than 0.05 is the estimated power.

Simulating the data works backward from what might be expected. Instead
starting with values for a risk factor (e.g., 200 0's and 1's
representing sex) and then simulating their disease status, we start
with the disease status (e.g., 50 affected cases and 150 controls with
135 unaffected and 15 affected) and then assign binary values for the
risk factor. It was done that way to control the nondetection rate and
group sizes.

The simulation algorithm is most easily described using examples, and we
begin with calculating power for the Fisher exact test under the
reference model, which is 100 cases and 100 correctly labeled (i.e.,
truly unaffected) controls. That is, there are no affected cases in the
control group, so this is not positive-unlabeled data, and the naive
model is the correct model. Next we associate a binary risk factor
variable, \(X\) (e.g., sex), with the cases and controls.The negative
controls were simulated by sampling 100 negative cases from a binomial
distribution with \(Pr(X=1) = 0.2\). That probability means the the
baseline risk for the disease in the population is 0.2. It it was chosen
arbitrarily, because the baseline risk isn't central to power, the
effect size is. As mentioned above, the effect size was 0.21, so the 100
positive cases were sampled from a binomial distribution with
\(Pr(X=1) = 0.2 + 0.21\).

Now, the 200 cases are pairs of binary data, one representing the group
and the other representing the risk factor. These simulated data were
tested with the Fisher exact test. As mentioned above, this process was
repeated 5000 times and the resulting 5000 p-values used to estimate
power.

For the second example, we calculate the power for a positive-unlabeled
example. Suppose that in a 100-patient control group, 10 percent are in
fact undetected positives. So the dataset is 100 affected cases in the
positive group, 10 affected cases in the control group, and 90
unaffected cases in the control group. As in the previous example, the
risk factor is simulated by sampling from binomial distributions. Now,
90 controls are sampled from the binomial distribution with
\(Pr(X=1) = 0.2\), the 10 affected controls are sampled from the
binomial distribution with \(Pr(X=1) = 0.2+ 0.21\), and 100 cases are
sampled from the same binomial distribution with
\(Pr(X=1) = 0.2 + 0.21\). The 10 mislabeled affected cases remain in the
control group, so as to measure the effect of treating PU data naively.
As before, the simulated data are treated like pilot data and power was
calculated. This process is repeated 5000 times and the result averaged.

\hypertarget{the-correction-factor}{%
\subsubsection{The Correction Factor}\label{the-correction-factor}}

The simulation algorithm was used to estimate the power of a Fisher's
exact test for a specific effect size for different sample sizes. For
aim 2, the sample-size correction factor, we used the same simulation
algorithm and effect size (0.21) but multiplied the group sample sizes
by possible correction factors, 1.1, 1.2, and so on, increasing sample
size and the power until it reached the 80\%.

\pagebreak

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

Table 1 describes power loss for the three sample sizes, (50, 150),
(150, 50), and (100, 100), and for three nondetection rates, 0, 0.05
(5\%), and 0.1 (10\%). To give these parameters context, if the group
sizes are (50, 150) and the nondectection rate is 0.1, then the positive
(affected) group has 50 cases, and the unlabeled control group (which we
are treating naively) has 150 cases, 15 of which are actually affected
cases. When the nondetection rate is zero, then the naive model is
correct because there no affected cases in the control group. The first
row is the reference power, so its loss of power compared to itself is
zero. The reference power was calculated in the simulation just like all
the other powers, and was estimated to be 0.82.

Columns 5 and 6 are the power loss columns and have negative entries
because for this simulation positive-unlabeled data analyzed under the
naive model always had lower power, as did unbalanced data. For example,
the second row shows a -4.81\% relative power reduction when the group
sizes are balanced but five percent of the control group is actually
positive cases.

Using Table 1, increasing nondetection rate typically reduces power.
However, group size imbalance can affect power loss more than relatively
small nondetection rates. For example, consider rows 3, 4, and 5. For
those rows, the nondetection rate is decreasing to zero, but power loss
is increasing due to group imbalance. However, within an experimented
with fixed group sizes (e.g., 100,100) increasing nondetection rate
alway means decreased power.

Table 1. Power loss.

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0339}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1441}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1441}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2034}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2034}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2712}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Row
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
N positive cases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
N naive controls
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nondetection proportion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Relative power loss (\%)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Absolute power loss (from 0.82)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 100 & 100 & 0.00 & 0.00 & 0.00 \\
2 & 100 & 100 & 0.05 & -4.81 & -0.04 \\
3 & 100 & 100 & 0.10 & -10.29 & -0.09 \\
4 & 50 & 150 & 0.00 & -10.45 & -0.09 \\
5 & 150 & 50 & 0.00 & -10.77 & -0.09 \\
6 & 150 & 50 & 0.05 & -14.92 & -0.13 \\
7 & 50 & 150 & 0.05 & -15.26 & -0.13 \\
8 & 50 & 150 & 0.10 & -22.47 & -0.20 \\
9 & 150 & 50 & 0.10 & -22.50 & -0.20 \\
\end{longtable}

Table 2. Power improvement. This table shows that increasing sample
sizes by about 40\% brings power back to the nominal rate. Rows 1, 6,
and 11, show the power for when there are no false positives. The other
rows show power when there is a 10\% nondection rate. The sixth column
shows the percent increase in sample size.
\begingroup\fontsize{11}{13}\selectfont

\resizebox{\linewidth}{!}{
\begin{tabular}{r|r|r|r|r|r|r}
\hline
Row & N positive cases & N naive controls & N false controls & N total & Percent increase & Power\\
\hline
\textbf{1} & \textbf{100} & \textbf{100} & \textbf{10} & \textbf{200} & \textbf{0.0} & \textbf{0.78}\\
\hline
2 & 110 & 110 & 11 & 220 & 10.0 & 0.83\\
\hline
3 & 120 & 120 & 12 & 240 & 20.0 & 0.87\\
\hline
4 & 130 & 130 & 13 & 260 & 30.0 & 0.89\\
\hline
5 & 140 & 140 & 14 & 280 & 40.0 & 0.90\\
\hline
6 & 150 & 50 & 5 & 200 & 0.0 & 0.67\\
\hline
7 & 165 & 55 & 6 & 220 & 10.0 & 0.70\\
\hline
8 & 180 & 60 & 6 & 240 & 20.0 & 0.77\\
\hline
9 & 195 & 79 & 8 & 274 & 37.0 & 0.84\\
\hline
10 & 210 & 80 & 8 & 290 & 45.0 & 0.87\\
\hline
11 & 50 & 150 & 15 & 200 & 0.0 & 0.69\\
\hline
12 & 55 & 165 & 16 & 220 & 10.0 & 0.73\\
\hline
13 & 60 & 180 & 18 & 240 & 20.0 & 0.78\\
\hline
14 & 70 & 195 & 20 & 265 & 32.5 & 0.83\\
\hline
15 & 80 & 210 & 21 & 290 & 45.0 & 0.85\\
\hline
\end{tabular}}
\endgroup{}

\pagebreak

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

This study showed that under specific conditions there was modest power
loss even for small proportions of undetected positives in the control
group. For example, with a 10 percent nondection rate and a balanced
study design, the sample size in the context of this simulation would
have to be XXXX, which is a zzz percent increase in sample size.

The working example was the association test for a single SNP in a GWAS
study, but the simulation results apply to any kind of study with
univariate association tests, such as any risk factor study. That is a
broad class of studies. Examples include the univariate associations
between post-op surgical infections and various surgical conditions
(e.g., boarded surgeon vs resident, manufacturer of bone plate). In that
case, there may be subdiagnostic infections in the control group.
Another example is univariate association tests in veterinary surveys,
such as XXX. In that case, there may be subclinical \ldots{}

In this simulaion, the undetected positives in the negative group were
randomly sampled from the same population as the detected positives in
the affected group. That's a common assumption (ref GU and others) but
there are other models. For example, the undetected positives in the
control group might be a subpopulation of positives defined by another
variable. For example, in a CCLD GWAS study, the undetected positives in
the control group might be positive dogs with low BCS only.

it does not apply to the situation where binary ``affectedness'' changes
as the function of a scaled risk factor variable. Using risk factors for
CCLD example, BCS may affect spontaneous rupture.

This was a simulation study that considered the effect on statistical
power of 10 percent or fewer undetected positives in the control group.
The simulation used

Other papers have discussed improved tests and power. (wang, and the pan
paper )

\newpage

\hypertarget{practical-implications}{%
\subsubsection{Practical implications}\label{practical-implications}}

However, the undetected-positive rate for CCLD GWAS studies is small,
certainly less that 10 percent, and the low rate causes only small
biases in odds ratio estimates. However, this study shows that unlabeled
data also affect the inferences, and we show that even a few positive
cases in the negative controls can affect the power of the study.
Fortunately, positive-unlabeled data appears to reduce power, making the
results reported in CCLD GWAS studies conservative.

\bibliography{references}
\bibliographystyle{plain}

\hypertarget{appendix-1.}{%
\subsection{Appendix 1.}\label{appendix-1.}}

Table 1. Sample sizes for four GWAS studies of CCLD. The last columns
shows the number of affected cases in the control group assuming 10
percent non-detection rate.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3182}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1061}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1061}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2424}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2273}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Study
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
N
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cases
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Naive controls
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Max No.~undetected positives (10\%)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Baird et al.~(2014) & 217 & 91 & 126 & 13 \\
Baker et al.~(2021) & 397 & 156 & 241 & 24 \\
Cook et al.~(2020) & 333 & 190 & 143 & 14 \\
Healy et al.~(2019) & 216 & 161 & 55 & 6 \\
\end{longtable}

\end{document}
